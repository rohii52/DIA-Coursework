{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Synchronous: 113 iterations\n",
      "Asynchronous: 172 iterations\n"
     ]
    }
   ],
   "source": [
    "# Code Implemented by,\n",
    "# Name: Rohith Ganesan\n",
    "# Id: 20553375\n",
    "# Mail: psxrg10@nottingham.ac.uk\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.table import Table\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "\n",
    "\n",
    "# Iterative Policy Evalution no Smal Gridworld\n",
    "# Gridworld is a common testbed environment for new RL algorithms.\n",
    "# We consider a small Gridsworld, a 4x4 grid of celLs,\n",
    "# where the northmost-westmost cell and the southmost-eastmost cell are terminal state \n",
    "# The agent can move between different cells.\n",
    "\n",
    "WORLD_SIZE = 4\n",
    "\n",
    "#Left, up, right, down\n",
    "ACTIONS = [np.array([0, -1]), np.array([-1, 0]), np.array([0, 1]), np.array([1, 0])]\n",
    "ACTION_PROB = 0.25\n",
    "\n",
    "def is_terminal(state):\n",
    "    x, y = state\n",
    "    return (x == 0 and y == 0) or (x == WORLD_SIZE - 1 and y == WORLD_SIZE - 1)\n",
    "\n",
    "def step(state, action):\n",
    "    if is_terminal(state):\n",
    "        return state, 0\n",
    "    next_state = (np.array(state) + action).tolist()\n",
    "    x, y = next_state\n",
    "    if x < 0 or x >= WORLD_SIZE or y < 0 or y >= WORLD_SIZE:\n",
    "        next_state = state\n",
    "    reward = -1\n",
    "    return next_state, reward\n",
    "\n",
    "def draw_image(image):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_axis_off()\n",
    "    tb = Table(ax, bbox=[0, 0, 1, 1])\n",
    "    nrows, ncols = image.shape\n",
    "    width, height = 1.0 / ncols, 1.0 / nrows\n",
    "\n",
    "    # Add cells\n",
    "    for (i, j), val in np.ndenumerate(image):\n",
    "        tb.add_cell(i, j, width, height, text=val, loc='center', facecolor='white')\n",
    "\n",
    "    # Row and column Labels...\n",
    "    for i in range(len(image)):\n",
    "        tb.add_cell(i, -1, width, height, text=i+1, loc='right', edgecolor='none', facecolor='none')\n",
    "        tb.add_cell(-1, i, width, height/2, text=i+1, loc='center', edgecolor='none', facecolor='none')\n",
    "\n",
    "    ax.add_table(tb)\n",
    "\n",
    "def compute_state_value(in_place=True, discount=1.0):\n",
    "    new_state_values = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        if in_place:\n",
    "            state_values = new_state_values\n",
    "        else:\n",
    "            state_values = new_state_values.copy()\n",
    "        old_state_values = state_values.copy()\n",
    "        for i in range(WORLD_SIZE):\n",
    "            for j in range(WORLD_SIZE):\n",
    "                value = 0\n",
    "                for action in ACTIONS:\n",
    "                    (next_i, next_j), reward = step([i, j], action)\n",
    "                    value += ACTION_PROB * (reward + discount * state_values[next_i, next_j])\n",
    "                new_state_values[i, j] = value\n",
    "        max_delta_value = abs(old_state_values - new_state_values).max()\n",
    "        if max_delta_value < 1e-4:\n",
    "            break\n",
    "        iteration += 1\n",
    "    return new_state_values, iteration\n",
    "\n",
    "def figure_4_1():\n",
    "    # While the author in the book (Richard S. Sutton, Andrew G Barto - Reinforcement Learning_ An Introduction, 2nd Edition-Bradford Books (2018)) suggests using in-place iterative policy evaluation,\n",
    "    # Figure 4.1 actually uses out-of-place version.\n",
    "    _, sync_iteration = compute_state_value(in_place=True)\n",
    "    values, async_iteration = compute_state_value(in_place=False)\n",
    "    draw_image(np.round(values, decimals=2))\n",
    "    print('Synchronous: {} iterations'.format(sync_iteration))\n",
    "    print('Asynchronous: {} iterations'.format(async_iteration))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    figure_4_1()\n",
    "    plt.savefig('grid_world_plot.png')\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PAPER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
